# ============================================
# LLM CONFIGURATION
# ============================================

# Local Model Configuration (Self-Hosted Mode)
# Set to 'true' to use Ollama (requires sufficient RAM - 4GB+ recommended)
# Set to 'false' to require cloud LLM configuration via UI
USE_LOCAL_MODELS=false

# Ollama Settings (only used if USE_LOCAL_MODELS=true)
EMBEDDING_SERVICE_URL=http://localhost:8001
OLLAMA_BASE_URL=http://localhost:11434
# Recommended models by memory:
# - llama3.2:1b (1.3GB) - Fastest, lowest memory
# - llama3.2 (2.0GB) - Good balance
# - llama3.2:3b (2.3GB) - Better quality
# - mistral (4.1GB) - High quality
OLLAMA_MODEL=llama3.2:1b  # Using 1B model for AWS Free Tier compatibility (1.3GB vs 2.3GB)

# Cloud LLM Configuration (OPTIONAL - can be set via UI)
# These are fallback defaults if not configured in UI
OPENAI_API_KEY=your_openai_api_key_here
GOOGLE_API_KEY=your_google_api_key_here

# ============================================
# DATABASE CONFIGURATION
# ============================================
DATABASE_URL=postgresql://raguser:ragpass@localhost:5432/ragdb

# ============================================
# APPLICATION SETTINGS
# ============================================
SECRET_KEY=your_secret_key_here
ENVIRONMENT=development

# ============================================
# AWS CONFIGURATION (for deployment)
# ============================================
AWS_REGION=us-east-1
AWS_ACCESS_KEY_ID=your_aws_key
AWS_SECRET_ACCESS_KEY=your_aws_secret
